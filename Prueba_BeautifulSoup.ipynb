{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Comment\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de carga del HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre un archivo HTML y devuelve su contenido como una cadena.\n",
    "def open_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de limpieza del HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina etiquetas específicas del HTML\n",
    "def erase_tags(soup):\n",
    "    try:\n",
    "        for script in soup(['footer','form','script','style','nav','img','i']):\n",
    "            if script is not None:\n",
    "                script.decompose()\n",
    "        clean_text=soup\n",
    "        return clean_text\n",
    "    \n",
    "    except Exception:\n",
    "        return soup\n",
    "\n",
    "def erase_comments(soup):\n",
    "    for element in soup(text=lambda text: isinstance(text, Comment)):\n",
    "        element.extract()\n",
    "    return soup\n",
    "\n",
    "def remove_isolated_links(soup):\n",
    "    for a_tag in soup.findAll('a'):\n",
    "        previous_sibling = a_tag.find_previous_sibling(string=True)\n",
    "        next_sibling = a_tag.find_next_sibling(string=True)\n",
    "\n",
    "        if not (previous_sibling and next_sibling):\n",
    "            a_tag.replace_with(NavigableString(a_tag.text))\n",
    "        else:\n",
    "            a_tag.decompose()\n",
    "            \n",
    "    return soup\n",
    "\n",
    "def href_for_text(soup):\n",
    "    a_tags = soup.find_all('a')\n",
    "    for tag in a_tags:\n",
    "        if '#0' in tag.get('href', ''): # Si la href del tag es igual a #0, eliminamos el tag entero.\n",
    "            tag.decompose()\n",
    "        else:  # Reemplazamos el enlace 'a' por el nuevo nodo que solo contiene el texto.\n",
    "            tag.replace_with(tag.text)\n",
    "    return soup\n",
    "\n",
    "def remove_specific_classes(soup, classes_to_remove):\n",
    "    for class_ in classes_to_remove:\n",
    "        for tag in soup.find_all(True, {'class': class_}):\n",
    "            tag.decompose()\n",
    "    return soup\n",
    "\n",
    "def remove_empty_structures(soup,parametros):\n",
    "    for param in parametros:\n",
    "        div_tags = soup.find_all(param)\n",
    "        for tag in div_tags:\n",
    "            if not tag.get_text(strip=True):\n",
    "                tag.decompose()\n",
    "    return soup\n",
    "\n",
    "def remove_specific_text(text_to_remove, soup):\n",
    "    for text_to_remove in text_to_remove:\n",
    "            for contenido in soup(text=lambda text: text_to_remove in text):\n",
    "                contenido.replace_with(contenido.replace(text_to_remove, ''))\n",
    "    return soup\n",
    "\n",
    "def clear_blankspaces_after_href(soup):\n",
    "    rgx=r'\\r\\n\\s*\\.'\n",
    "    new_str_soup=re.sub(rgx,'',str(soup))\n",
    "    soup= BeautifulSoup(new_str_soup, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de conversión de tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase_empty_lines(clean_text):\n",
    "    lines = (line.strip() for line in clean_text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    clean_text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return clean_text\n",
    "\n",
    "def html_to_text(soup):\n",
    "    texto=soup.get_text()\n",
    "    texto_normalizado=erase_empty_lines(texto)\n",
    "    return texto_normalizado\n",
    "\n",
    "def soup_to_html(soup):\n",
    "    try:\n",
    "        html=soup.prettify()\n",
    "        return html\n",
    "    except Exception:\n",
    "        return soup\n",
    "\n",
    "def html_to_soup(filepath):\n",
    "    html_content = open_html(filepath)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser') \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txt(text, titulo, tipo):\n",
    "    if tipo != 'texto':\n",
    "        text=soup_to_html(text)\n",
    "    archivo = open(f\"{titulo}.txt\", \"w\",encoding='utf-8')\n",
    "    archivo.write(text)\n",
    "    archivo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sacar texto limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_html_scrapping(file_path):\n",
    "    soup = html_to_soup(file_path)\n",
    "    content_l = erase_tags(soup)\n",
    "    content_l = remove_isolated_links(content_l)\n",
    "    content_sin_comments = erase_comments(content_l)\n",
    "    content_l = href_for_text(content_sin_comments)\n",
    "    # content_l = remove_specific_text([''], content_l) # para borrar algun texto especifico, como de la cabecera o de la parte de abajo\n",
    "    # content_l = remove_specific_classes(content_l, [''])\n",
    "    content_l = remove_empty_structures(content_l, ['span','p','div']) # eliminar todas las span vacias\n",
    "    content_l = clear_blankspaces_after_href(content_l) # eliminamos el salto de linea que se mete cuando cambiamos las href por texto\n",
    "    return content_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_scrapping(file_path, file_name):\n",
    "    soup = main_html_scrapping(file_path)\n",
    "    texto = html_to_text(soup)\n",
    "    save_txt(texto, file_name, 'texto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/BOE-A-1902-8161.html' \n",
    "main_scrapping(file_path, 'BOE-A-1902-8161.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
